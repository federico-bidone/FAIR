"""Pipeline orchestrator for the regime probability engine."""

from __future__ import annotations

from collections.abc import Mapping
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd
from pandas.api.types import is_categorical_dtype, is_object_dtype

from fair3.engine.logging import setup_logger
from fair3.engine.regime.committee import regime_probability
from fair3.engine.utils.io import artifact_path, read_yaml
from fair3.engine.utils.rand import generator_from_seed

LOG = setup_logger(__name__)


@dataclass(slots=True)
class RegimePipelineResult:
    """Artefacts generated by the regime probability pipeline.

    Attributes:
      scores: DataFrame with regime probabilities and component diagnostics.
      probabilities_path: CSV path containing the full probability panel.
      hysteresis_path: CSV path containing the hysteresis regime flag.
      committee_log_path: CSV path logging the individual committee components.
      thresholds: Mapping with the thresholds applied during the run.
    """

    scores: pd.DataFrame
    probabilities_path: Path
    hysteresis_path: Path
    committee_log_path: Path
    thresholds: Mapping[str, Any]


def _load_thresholds(path: Path) -> Mapping[str, Any]:
    """Load the thresholds YAML returning a mapping."""

    payload = read_yaml(path)
    if isinstance(payload, Mapping):
        return payload
    LOG.warning("Threshold file %s is not a mapping; using defaults", path)
    return {}


def _load_returns(clean_root: Path, seed: int) -> pd.DataFrame:
    """Load returns from the clean panel or synthesise deterministic data."""

    returns_path = clean_root / "returns.parquet"
    if returns_path.exists():
        returns = pd.read_parquet(returns_path)
        if isinstance(returns.index, pd.MultiIndex):
            index_names = [name or f"level_{idx}" for idx, name in enumerate(returns.index.names)]
            returns_reset = returns.reset_index()
            date_col = index_names[0]
            if date_col not in returns_reset.columns:
                date_col = returns_reset.columns[0]
            returns_reset[date_col] = pd.to_datetime(returns_reset[date_col])

            symbol_col: str
            if len(index_names) > 1 and index_names[1] in returns_reset.columns:
                symbol_col = index_names[1]
            else:
                symbol_candidates = [col for col in returns_reset.columns if col not in {date_col}]
                if not symbol_candidates:
                    raise ValueError("Returns parquet missing symbol column")
                symbol_col = next(
                    (
                        col
                        for col in symbol_candidates
                        if is_object_dtype(returns_reset[col])
                        or is_categorical_dtype(returns_reset[col])
                    ),
                    symbol_candidates[0],
                )

            preferred_fields = ["log_ret", "ret", "return"]
            value_col = next(
                (field for field in preferred_fields if field in returns_reset.columns),
                None,
            )
            if value_col is None:
                candidate_fields = [
                    col for col in returns_reset.columns if col not in {date_col, symbol_col}
                ]
                if not candidate_fields:
                    raise ValueError("Returns parquet missing value columns")
                value_col = candidate_fields[0]

            try:
                wide = returns_reset.pivot(index=date_col, columns=symbol_col, values=value_col)
            except ValueError:
                wide = returns_reset.pivot_table(
                    index=date_col,
                    columns=symbol_col,
                    values=value_col,
                    aggfunc="last",
                )
            wide = wide.sort_index()
            return wide

        returns.index = pd.to_datetime(returns.index)
        if isinstance(returns.columns, pd.MultiIndex):
            preferred_fields = ["log_ret", "ret", "return"]
            for field in preferred_fields:
                if field in returns.columns.get_level_values(0):
                    return returns[field]
            first_field = returns.columns.get_level_values(0)[0]
            return returns[first_field]
        return returns

    LOG.warning("returns.parquet missing at %s; creating synthetic panel", returns_path)
    idx = pd.date_range("2015-01-01", periods=252, freq="B")
    rng = generator_from_seed(seed)
    columns = ["SYN_EQ", "SYN_BND", "SYN_ALT"]
    data = rng.normal(0.0006, 0.01, size=(len(idx), len(columns)))
    return pd.DataFrame(data, index=idx, columns=columns)


def _load_volatility(clean_root: Path, returns: pd.DataFrame) -> pd.Series:
    """Load realised volatility proxy from features or derive from returns."""

    features_path = clean_root / "features.parquet"
    if features_path.exists():
        features = pd.read_parquet(features_path)
        if isinstance(features.index, pd.MultiIndex):
            index_names = [name or f"level_{idx}" for idx, name in enumerate(features.index.names)]
            features_reset = features.reset_index()
            date_col = index_names[0]
            if date_col not in features_reset.columns:
                date_col = features_reset.columns[0]
            features_reset[date_col] = pd.to_datetime(features_reset[date_col])
            if "lag_vol_21" in features_reset.columns:
                symbol_col = None
                if len(index_names) > 1 and index_names[1] in features_reset.columns:
                    symbol_col = index_names[1]
                else:
                    symbol_candidates = [
                        col for col in features_reset.columns if col not in {date_col, "lag_vol_21"}
                    ]
                    if symbol_candidates:
                        symbol_col = next(
                            (
                                col
                                for col in symbol_candidates
                                if is_object_dtype(features_reset[col])
                                or is_categorical_dtype(features_reset[col])
                            ),
                            symbol_candidates[0],
                        )
                if symbol_col is not None:
                    try:
                        vol_frame = features_reset.pivot(
                            index=date_col, columns=symbol_col, values="lag_vol_21"
                        )
                    except ValueError:
                        vol_frame = features_reset.pivot_table(
                            index=date_col,
                            columns=symbol_col,
                            values="lag_vol_21",
                            aggfunc="last",
                        )
                    vol_frame = vol_frame.sort_index()
                    return vol_frame.mean(axis=1)
                series = features_reset.set_index(date_col)["lag_vol_21"]
                return series.sort_index()
        elif "lag_vol_21" in features.columns:
            features.index = pd.to_datetime(features.index)
            return features["lag_vol_21"]

    LOG.info("lag_vol_21 not available; estimating realised volatility from returns")
    window = 21
    vol = returns.rolling(window=window, min_periods=max(5, window // 2)).std()
    return vol.mean(axis=1) * (252**0.5)


def _synthesise_macro(index: pd.DatetimeIndex, seed: int) -> pd.DataFrame:
    """Generate deterministic macro series when real data is unavailable."""

    rng = generator_from_seed(seed + 42)
    trend = np.linspace(0.018, 0.023, len(index))
    inflation = trend + rng.normal(0.0, 0.0005, len(index))
    pmi = 52.0 - rng.normal(0.0, 0.8, len(index)).cumsum() / 20.0
    real_rate = -0.005 + rng.normal(0.0, 0.0007, len(index))
    return pd.DataFrame(
        {
            "inflation_yoy": inflation,
            "pmi": pmi,
            "real_rate": real_rate,
        },
        index=index,
    )


def _load_macro(clean_root: Path, index: pd.DatetimeIndex, seed: int) -> pd.DataFrame:
    """Load macro features if present or synthesise deterministic placeholders."""

    features_path = clean_root / "features.parquet"
    if features_path.exists():
        features = pd.read_parquet(features_path)
        if isinstance(features.index, pd.MultiIndex):
            index_names = [name or f"level_{idx}" for idx, name in enumerate(features.index.names)]
            features_reset = features.reset_index()
            date_col = index_names[0]
            if date_col not in features_reset.columns:
                date_col = features_reset.columns[0]
            features_reset[date_col] = pd.to_datetime(features_reset[date_col])
            selected = [
                col
                for col in features_reset.columns
                if col in {"inflation_yoy", "pmi", "real_rate"}
            ]
            if selected:
                grouped = (
                    features_reset[[date_col] + selected]
                    .groupby(date_col, sort=True)
                    .mean(numeric_only=True)
                )
                if not grouped.empty:
                    grouped.index.name = "date"
                    return grouped
        else:
            features.index = pd.to_datetime(features.index)
            if not isinstance(features.columns, pd.MultiIndex):
                selected = [
                    col for col in features.columns if col in {"inflation_yoy", "pmi", "real_rate"}
                ]
                if selected:
                    subset = features[selected]
                    if not subset.empty:
                        return subset
    LOG.info("Macro features unavailable; using synthetic deterministic series")
    return _synthesise_macro(index, seed)


def _assemble_panel(clean_root: Path, seed: int) -> pd.DataFrame:
    """Assemble the multi-field panel consumed by the regime engine."""

    returns = _load_returns(clean_root, seed)
    vol = _load_volatility(clean_root, returns)
    macro = _load_macro(clean_root, returns.index, seed)

    frames = [pd.concat({"returns": returns}, axis=1)]
    if not vol.empty:
        frames.append(pd.concat({"volatility": vol.to_frame(name="vol")}, axis=1))
    if not macro.empty:
        frames.append(pd.concat({"macro": macro}, axis=1))
    panel = pd.concat(frames, axis=1)
    return panel.sort_index()


def run_regime_pipeline(
    *,
    clean_root: Path | str | None = None,
    thresholds_path: Path | str = Path("configs") / "thresholds.yml",
    output_dir: Path | str | None = None,
    seed: int = 0,
    dry_run: bool = False,
    trace: bool = False,
) -> RegimePipelineResult:
    """Run the regime probability engine and persist diagnostic artefacts."""

    clean_root = Path(clean_root) if clean_root is not None else Path("data") / "clean"
    thresholds_path = Path(thresholds_path)
    output_root = Path(output_dir) if output_dir is not None else None

    LOG.info("Starting regime pipeline clean_root=%s thresholds=%s", clean_root, thresholds_path)
    panel = _assemble_panel(clean_root, seed)
    thresholds = _load_thresholds(thresholds_path)
    scores = regime_probability(panel, thresholds, seed)
    if scores.empty:
        raise ValueError("Regime probability computation returned an empty frame")

    probabilities_path = artifact_path("regime", "probabilities.csv", root=output_root)
    scores.to_csv(probabilities_path, index_label="date")

    hysteresis_path = artifact_path("regime", "hysteresis.csv", root=output_root)
    scores[["regime_flag"]].to_csv(hysteresis_path, index_label="date")

    committee_columns = [col for col in ("p_hmm", "p_volatility", "p_macro") if col in scores]
    committee_log = scores[committee_columns].copy()
    committee_log["p_crisis"] = scores["p_crisis"]
    committee_log_path = artifact_path("regime", "committee_log.csv", root=output_root)
    committee_log.to_csv(committee_log_path, index_label="date")

    if trace:
        preview = scores.tail(5).to_string(float_format=lambda x: f"{x:.3f}")
        LOG.info("Regime preview (tail=5):\n%s", preview)

    LOG.info(
        "Regime pipeline complete dry_run=%s artifacts=(%s, %s, %s)",
        dry_run,
        probabilities_path,
        hysteresis_path,
        committee_log_path,
    )
    return RegimePipelineResult(
        scores=scores,
        probabilities_path=probabilities_path,
        hysteresis_path=hysteresis_path,
        committee_log_path=committee_log_path,
        thresholds=thresholds.get("regime", thresholds),
    )


__all__ = ["RegimePipelineResult", "run_regime_pipeline"]
