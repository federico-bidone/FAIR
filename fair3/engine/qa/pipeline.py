"""Deterministic QA pipeline that exercises the FAIR-III stack end-to-end.

The helpers here synthesise raw CSV inputs, invoke the ingest/ETL/factor/
estimate/optimise/map/regime/report stages, and surface acceptance plus
robustness diagnostics.  They intentionally avoid network access so that
regressions can be reproduced offline.
"""

from __future__ import annotations

from collections.abc import Mapping
from dataclasses import dataclass
from pathlib import Path

import numpy as np
import pandas as pd

from fair3.engine.allocators.pipeline import run_optimization_pipeline
from fair3.engine.estimates.pipeline import run_estimate_pipeline
from fair3.engine.etl import TRPanelBuilder
from fair3.engine.factors.pipeline import run_factor_pipeline
from fair3.engine.logging import setup_logger
from fair3.engine.mapping.pipeline import run_mapping_pipeline
from fair3.engine.regime.pipeline import run_regime_pipeline
from fair3.engine.reporting.monthly import (
    MonthlyReportInputs,
    generate_monthly_report,
)
from fair3.engine.robustness import RobustnessConfig, run_robustness_lab
from fair3.engine.utils.io import ARTIFACTS_ROOT, ensure_dir, read_yaml
from fair3.engine.utils.rand import generator_from_seed, spawn_child_rng

LOG = setup_logger(__name__)


@dataclass(frozen=True)
class DemoInstrumentSpec:
    """Specification used to generate synthetic price paths for QA.

    Attributes:
      symbol: Instrument identifier used across the pipeline.
      mean: Average daily log-return applied during path generation.
      volatility: Daily log-return volatility parameter.
    """

    symbol: str
    mean: float
    volatility: float


@dataclass(frozen=True)
class DemoQAConfig:
    """Configuration driving the deterministic QA pipeline.

    Attributes:
      label: Human readable label used for directory names and reporting.
      start: Inclusive start date for the synthetic business-day calendar.
      end: Inclusive end date for the synthetic business-day calendar.
      instruments: Collection of instrument specifications used to craft
        synthetic CSV inputs for the ETL builder.
      base_currency: Portfolio reporting currency applied during ETL.
      output_dir: Optional custom root for QA artefacts. When omitted the
        pipeline stores files under ``artifacts/qa/<label>``.
      seed: Optional explicit seed overriding the deterministic stream.
      seed_stream: Seed stream name looked up in ``audit/seeds.yml`` when
        ``seed`` is ``None``.
      validate_factors: Whether to execute cross-validation during factor
        construction (disabling it speeds up CI runs).
      cv_splits: Number of folds applied to stochastic estimators.
      robustness_draws: Number of bootstrap draws for the robustness lab.
      robustness_block_size: Block size in days for bootstrap sampling.
      robustness_alpha: Confidence level used by the robustness gates.
      cagr_target: Acceptance gate lower bound for CAGR.
      max_drawdown_threshold: Acceptance gate threshold for drawdowns.
    """

    label: str = "demo"
    start: pd.Timestamp = pd.Timestamp("2018-01-01")
    end: pd.Timestamp = pd.Timestamp("2021-12-31")
    instruments: tuple[DemoInstrumentSpec, ...] = (
        DemoInstrumentSpec("DEMO_EQ", 0.0006, 0.012),
        DemoInstrumentSpec("DEMO_BOND", 0.0002, 0.004),
        DemoInstrumentSpec("DEMO_ALT", 0.0004, 0.015),
    )
    base_currency: str = "EUR"
    output_dir: Path | None = None
    seed: int | None = None
    seed_stream: str = "qa"
    validate_factors: bool = False
    cv_splits: int = 3
    robustness_draws: int = 256
    robustness_block_size: int = 45
    robustness_alpha: float = 0.95
    cagr_target: float = 0.03
    max_drawdown_threshold: float = -0.25


@dataclass(frozen=True)
class DemoQAResult:
    """Summary artefacts produced by :func:`run_demo_qa`.

    Attributes:
      qa_root: Root directory that groups all generated artefacts.
      raw_root: Directory containing the synthetic raw CSV files.
      clean_root: Directory containing the PIT-cleaned Parquet panel.
      artifacts_root: Directory hosting factor, estimate, mapping outputs.
      report_pdf: Path to the generated monthly PDF report.
      acceptance_json: Acceptance gate summary JSON path.
      robustness_report: PDF generated by the robustness laboratory.
      robustness_summary: JSON file with robustness gate statistics.
      ablation_csv: CSV describing the ablation deltas for governance features.
      acceptance_passed: ``True`` when acceptance gates pass.
      robustness_passed: ``True`` when robustness gates pass.
    """

    qa_root: Path
    raw_root: Path
    clean_root: Path
    artifacts_root: Path
    report_pdf: Path
    acceptance_json: Path
    robustness_report: Path
    robustness_summary: Path
    ablation_csv: Path
    acceptance_passed: bool
    robustness_passed: bool


def _price_path(
    dates: pd.DatetimeIndex,
    *,
    spec: DemoInstrumentSpec,
    rng: np.random.Generator,
    base_price: float = 100.0,
) -> pd.Series:
    """Generate a synthetic log-normal price path.

    Args:
      dates: Business-day calendar covering the desired interval.
      spec: Instrument specification with mean/volatility parameters.
      rng: Deterministic NumPy generator providing randomness.
      base_price: Starting price applied to the synthetic path.

    Returns:
      Series indexed by ``dates`` containing the simulated price path.
    """

    log_returns = rng.normal(spec.mean, spec.volatility, size=len(dates))
    cumulative = np.cumsum(log_returns, dtype="float64")
    prices = base_price * np.exp(cumulative)
    return pd.Series(prices, index=dates, name=spec.symbol)


def _write_raw_data(
    config: DemoQAConfig,
    *,
    rng: np.random.Generator,
    raw_root: Path,
) -> None:
    """Materialise synthetic CSV files expected by :class:`TRPanelBuilder`.

    Args:
      config: QA configuration containing instrument specifications.
      rng: Deterministic generator used to craft independent paths.
      raw_root: Destination directory for the generated CSV files.
    """

    ensure_dir(raw_root)
    dates = pd.date_range(config.start, config.end, freq="B")
    source_dir = ensure_dir(raw_root / config.label)
    for spec in config.instruments:
        series = _price_path(dates, spec=spec, rng=rng)
        frame = pd.DataFrame(
            {
                "date": series.index,
                "value": series.to_numpy(),
                "symbol": spec.symbol,
                "currency": config.base_currency,
            }
        )
        path = source_dir / f"{spec.symbol.lower()}.csv"
        frame.to_csv(path, index=False)
        LOG.debug("Scritto CSV sintetico %s", path)


def _portfolio_returns(
    instrument_returns: pd.DataFrame,
    weights: pd.Series,
) -> pd.Series:
    """Compute portfolio returns from instrument panel and static weights.

    Args:
      instrument_returns: Wide return panel indexed by date.
      weights: Target instrument weights aligned to the panel columns.

    Returns:
      Series of portfolio returns obtained by multiplying weights and
      instrument returns.
    """

    aligned_weights = weights.reindex(instrument_returns.columns).fillna(0.0)
    portfolio = instrument_returns.mul(aligned_weights, axis=1).sum(axis=1)
    return portfolio.astype("float64")


def _instrument_weights_frame(
    index: pd.DatetimeIndex,
    weights: pd.Series,
) -> pd.DataFrame:
    """Broadcast instrument weights across the reporting horizon.

    Args:
      index: Date index used by the reporting time series.
      weights: Static instrument weights produced by the mapping pipeline.

    Returns:
      DataFrame repeating the target weights for each timestamp in ``index``.
    """

    matrix = np.tile(weights.to_numpy(dtype="float64"), (len(index), 1))
    frame = pd.DataFrame(matrix, index=index, columns=weights.index)
    return frame


def _factor_contributions(
    factors: pd.DataFrame,
    factor_weights: pd.Series,
) -> pd.DataFrame:
    """Compute factor contributions using target factor weights.

    Args:
      factors: Factor return panel indexed by date.
      factor_weights: Target weights for each factor.

    Returns:
      DataFrame where each column represents the contribution of a factor
      given the requested allocation.
    """

    aligned = factor_weights.reindex(factors.columns).fillna(0.0)
    return factors.mul(aligned, axis=1)


def _instrument_contributions(
    instrument_returns: pd.DataFrame,
    weights: pd.Series,
) -> pd.DataFrame:
    """Derive instrument attribution from returns and weights.

    Args:
      instrument_returns: Wide return panel indexed by date.
      weights: Instrument weights aligned with the return columns.

    Returns:
      DataFrame capturing per-instrument contribution to portfolio returns.
    """

    aligned = weights.reindex(instrument_returns.columns).fillna(0.0)
    return instrument_returns.mul(aligned, axis=1)


def _turnover_series(weights: pd.DataFrame) -> pd.Series:
    """Approximate realised turnover from the weight time series.

    Args:
      weights: Time series of instrument weights.

    Returns:
      Series representing absolute daily turnover.
    """

    diffs = weights.diff().abs().sum(axis=1)
    return diffs.fillna(0.0)


def _ablation_runner(
    returns: pd.Series,
    *,
    penalties: Mapping[str, float],
) -> Mapping[str, float]:
    """Build an ablation callback that perturbs returns for disabled features.

    Args:
      returns: Portfolio return series used as baseline.
      penalties: Mapping of governance feature name to multiplicative penalty.

    Returns:
      Callable suitable for :func:`run_robustness_lab` that produces metric
      deltas when features are toggled.
    """

    def _runner(flags: Mapping[str, bool], seed: int | None = None) -> Mapping[str, float]:
        scale = 1.0
        for name, penalty in penalties.items():
            if not flags.get(name, True):
                scale *= penalty
        rng = np.random.default_rng(seed)
        noise = rng.normal(0.0, 0.0005, size=len(returns))
        adjusted = returns.to_numpy(copy=False) * scale + noise
        mean = float(np.mean(adjusted))
        std = float(np.std(adjusted, ddof=0))
        sharpe = mean / std if std > 0 else 0.0
        wealth = np.cumprod(1.0 + adjusted)
        peaks = np.maximum.accumulate(wealth)
        drawdowns = wealth / peaks - 1.0
        max_drawdown = float(np.min(drawdowns))
        return {"sharpe": sharpe, "max_drawdown": max_drawdown}

    return _runner


def run_demo_qa(config: DemoQAConfig | None = None) -> DemoQAResult:
    """Execute the deterministic QA pipeline on a synthetic dataset.

    Args:
      config: Optional configuration overriding defaults such as date range,
        output directory, and bootstrap parameters.

    Returns:
      ``DemoQAResult`` containing references to the generated artefacts and the
      evaluation outcome of acceptance and robustness gates.

    Raises:
      FileNotFoundError: If mandatory configuration files are missing.
      ValueError: If downstream pipeline components encounter invalid inputs.
    """

    cfg = config or DemoQAConfig()
    seed = cfg.seed if cfg.seed is not None else None
    rng_main = generator_from_seed(seed, stream=cfg.seed_stream)
    raw_rng = spawn_child_rng(rng_main)
    qa_root = ensure_dir(cfg.output_dir or (ARTIFACTS_ROOT / "qa" / cfg.label))
    raw_root = ensure_dir(qa_root / "raw")
    clean_root = ensure_dir(qa_root / "clean")
    artifacts_root = ensure_dir(qa_root / "artifacts")
    audit_root = ensure_dir(qa_root / "audit")

    _write_raw_data(cfg, rng=raw_rng, raw_root=raw_root)
    builder = TRPanelBuilder(
        raw_root=raw_root,
        clean_root=clean_root,
        audit_root=audit_root,
        base_currency=cfg.base_currency,
    )
    etl_seed = int(rng_main.integers(0, 2**32 - 1))
    builder.build(seed=etl_seed)

    factor_seed = int(rng_main.integers(0, 2**32 - 1))
    run_factor_pipeline(
        clean_root=clean_root,
        artifacts_root=artifacts_root,
        audit_dir=audit_root,
        seed=factor_seed,
        validate=cfg.validate_factors,
        oos_splits=max(2, cfg.cv_splits),
    )

    estimate_seed = int(rng_main.integers(0, 2**32 - 1))
    run_estimate_pipeline(
        artifacts_root=artifacts_root,
        audit_dir=audit_root,
        seed=estimate_seed,
        cv_splits=max(2, cfg.cv_splits),
        sigma_engine="spd_median",
    )

    run_optimization_pipeline(
        artifacts_root=artifacts_root,
        audit_dir=audit_root,
    )

    mapping_result = run_mapping_pipeline(
        artifacts_root=artifacts_root,
        clean_root=clean_root,
        audit_dir=audit_root,
    )

    regime_seed = int(rng_main.integers(0, 2**32 - 1))
    run_regime_pipeline(
        clean_root=clean_root,
        output_dir=artifacts_root,
        seed=regime_seed,
    )

    asset_panel = pd.read_parquet(clean_root / "asset_panel.parquet")
    pivot = asset_panel.pivot_table(index=["date", "symbol"], columns="field", values="value")
    pivot.index = pd.MultiIndex.from_tuples(
        [
            (
                pd.to_datetime(idx[0]).tz_convert("Europe/Rome").tz_localize(None),
                idx[1],
            )
            for idx in pivot.index
        ],
        names=["date", "symbol"],
    )
    instrument_returns = pivot["ret"].unstack(level="symbol").sort_index().fillna(0.0)

    instrument_weights = pd.read_csv(mapping_result.instrument_weights_path, index_col=0).iloc[:, 0]
    weights_frame = _instrument_weights_frame(instrument_returns.index, instrument_weights)
    portfolio_returns = _portfolio_returns(instrument_returns, instrument_weights)

    factors = pd.read_parquet(artifacts_root / "factors" / "factors.parquet")
    factors.index = pd.to_datetime(factors.index)
    factors = factors.reindex(instrument_returns.index).ffill().fillna(0.0)
    factor_allocation = pd.read_csv(
        artifacts_root / "weights" / "factor_allocation.csv", index_col=0
    ).iloc[:, 0]
    factor_contrib = _factor_contributions(factors, factor_allocation)
    instrument_contrib = _instrument_contributions(instrument_returns, instrument_weights)

    turnover = _turnover_series(weights_frame)
    costs = turnover * 0.0005
    taxes = turnover * 0.0002

    thresholds_cfg = read_yaml(Path("configs") / "thresholds.yml")
    execution_cfg = (
        thresholds_cfg.get("execution", {}) if isinstance(thresholds_cfg, Mapping) else {}
    )
    te_cap = float(execution_cfg.get("TE_max_factor", 0.02))
    mapping_summary = read_yaml(mapping_result.summary_path) or {}
    max_factor_dev = float(mapping_summary.get("max_factor_deviation", 0.0))
    compliance_flags = {
        "turnover_cap": float(execution_cfg.get("turnover_cap", 0.4)) >= float(turnover.sum()),
        "te_factor_cap": max_factor_dev <= te_cap + 1e-9,
        "ucits": True,
    }

    penalties = {
        name: 0.9
        for name in (
            "bl_fallback",
            "sigma_psd",
            "drift_trigger",
            "meta_to_te",
            "regime_tilt",
            "no_trade_rule",
        )
    }
    robustness_config = RobustnessConfig(
        draws=cfg.robustness_draws,
        block_size=cfg.robustness_block_size,
        alpha=cfg.robustness_alpha,
        cagr_target=cfg.cagr_target,
        max_drawdown_threshold=cfg.max_drawdown_threshold,
        output_dir=qa_root / "robustness",
    )
    artifacts, gates = run_robustness_lab(
        portfolio_returns,
        config=robustness_config,
        seed=int(rng_main.integers(0, 2**32 - 1)),
        ablation_runner=_ablation_runner(portfolio_returns, penalties=penalties),
    )
    bootstrap_metrics = pd.read_csv(artifacts.bootstrap_csv)

    thresholds = {
        "max_drawdown_threshold": cfg.max_drawdown_threshold,
        "cagr_target": cfg.cagr_target,
    }

    report_inputs = MonthlyReportInputs(
        returns=portfolio_returns,
        weights=weights_frame,
        factor_contributions=factor_contrib,
        instrument_contributions=instrument_contrib,
        turnover=turnover,
        costs=costs,
        taxes=taxes,
        compliance_flags=compliance_flags,
        cluster_map={
            "Growth": [cfg.instruments[0].symbol],
            "Income": [cfg.instruments[1].symbol],
            "Diversifier": [cfg.instruments[-1].symbol],
        },
        instrument_returns=instrument_returns,
        factor_returns=factors,
        bootstrap_metrics=bootstrap_metrics,
        thresholds=thresholds,
    )

    period_label = f"{cfg.start.date()}_{cfg.end.date()}"
    report_artifacts = generate_monthly_report(
        report_inputs,
        period_label=period_label,
        output_dir=qa_root / "reports",
        seed=int(rng_main.integers(0, 2**32 - 1)),
    )

    acceptance_payload = read_yaml(report_artifacts.acceptance_json)
    acceptance_passed = (
        bool(acceptance_payload.get("passes", False))
        if isinstance(acceptance_payload, Mapping)
        else False
    )

    robustness_payload = read_yaml(artifacts.summary_json)
    robustness_passed = (
        bool(robustness_payload.get("passes", False))
        if isinstance(robustness_payload, Mapping)
        else gates.passes()
    )

    return DemoQAResult(
        qa_root=qa_root,
        raw_root=raw_root,
        clean_root=clean_root,
        artifacts_root=artifacts_root,
        report_pdf=report_artifacts.report_pdf,
        acceptance_json=report_artifacts.acceptance_json,
        robustness_report=artifacts.report_pdf,
        robustness_summary=artifacts.summary_json,
        ablation_csv=artifacts.ablation_csv or artifacts.summary_json,
        acceptance_passed=acceptance_passed,
        robustness_passed=robustness_passed,
    )
